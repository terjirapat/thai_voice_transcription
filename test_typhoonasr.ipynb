{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202aba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import signal\n",
    "\n",
    "# Windows does not have SIGKILL\n",
    "if sys.platform == \"win32\":\n",
    "    signal.SIGKILL = signal.SIGTERM  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4900f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Typhoon ASR Real-Time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-18 16:27:40 nemo_logging:393] d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\TDGE0049\\.cache\\huggingface\\hub\\models--scb10x--typhoon-asr-realtime. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "    To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "      warnings.warn(message)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-18 16:27:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 2048 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-18 16:27:44 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/workspace/warit/nemo-asr/stt_th_conformer_transducer_large/prepare_data/typhoon_cleanser/20250814/Split_gg/train_data_typhoon_asr_realtime.jsonl\n",
      "    sample_rate: 16000\n",
      "    batch_size: 8\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 30.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-11-18 16:27:44 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /data/workspace/warit/nemo-asr/data/scbx_testset_manifest.jsonl\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    max_duration: 30.0\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    \n",
      "[NeMo W 2025-11-18 16:27:44 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-18 16:27:44 nemo_logging:381] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-18 16:27:45 nemo_logging:393] d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-18 16:27:45 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-11-18 16:27:45 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-18 16:27:45 nemo_logging:393] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-18 16:27:45 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-18 16:27:45 nemo_logging:393] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-18 16:27:46 nemo_logging:381] Model EncDecRNNTBPEModel was successfully restored from C:\\Users\\TDGE0049\\.cache\\huggingface\\hub\\models--scb10x--typhoon-asr-realtime\\snapshots\\a14b79d50c788dbdfe559c8a28a9b90153cf3865\\typhoon-asr-realtime.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "import torch\n",
    "\n",
    "# Select processing device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Typhoon ASR Real-Time model\n",
    "print(\"Loading Typhoon ASR Real-Time...\")\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(\n",
    "    model_name=\"scb10x/typhoon-asr-realtime\",\n",
    "    map_location=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Start capturing (Ctrl+C to stop)\n",
      "Audio block: [ 0.0000000e+00  0.0000000e+00 -3.0517578e-05  0.0000000e+00\n",
      "  0.0000000e+00]\n",
      "Audio block: [0. 0. 0. 0. 0.]\n",
      "Audio block: [-0.00305176 -0.00296021 -0.00314331 -0.00323486 -0.0032959 ]\n",
      "Audio block: [-0.01605225 -0.01608276 -0.01617432 -0.01617432 -0.01623535]\n",
      "Audio block: [-0.01312256 -0.01306152 -0.01306152 -0.01303101 -0.01296997]\n",
      "Audio block: [-0.00753784 -0.00756836 -0.00762939 -0.00765991 -0.00765991]\n",
      "Audio block: [-0.00592041 -0.00595093 -0.00592041 -0.00595093 -0.00588989]\n",
      "Audio block: [-0.00393677 -0.00390625 -0.00396729 -0.0039978  -0.00384521]\n",
      "Audio block: [0.00137329 0.00131226 0.00125122 0.0012207  0.00125122]\n",
      "Audio block: [0.00442505 0.00445557 0.00445557 0.00454712 0.00445557]\n",
      "Audio block: [0.00460815 0.00463867 0.00457764 0.00466919 0.00469971]\n",
      "Audio block: [0.0050354  0.00509644 0.00515747 0.00521851 0.00515747]\n",
      "Audio block: [0.00506592 0.00509644 0.0050354  0.00506592 0.00491333]\n",
      "Audio block: [0.00439453 0.00439453 0.00445557 0.00457764 0.00448608]\n",
      "Audio block: [0.00387573 0.0038147  0.00378418 0.00390625 0.00390625]\n",
      "Audio block: [0.00253296 0.00250244 0.00241089 0.00241089 0.00241089]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sd.InputStream(samplerate=SAMPLE_RATE, channels=\u001b[32m1\u001b[39m, dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m                     blocksize=BLOCK_SIZE, callback=audio_callback):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         \u001b[43msd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\sounddevice.py:712\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(msec)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(msec):\n\u001b[32m    706\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Put the caller to sleep for at least *msec* milliseconds.\u001b[39;00m\n\u001b[32m    707\u001b[39m \n\u001b[32m    708\u001b[39m \u001b[33;03m    The function may sleep longer than requested so don't rely on this\u001b[39;00m\n\u001b[32m    709\u001b[39m \u001b[33;03m    for accurate musical timing.\u001b[39;00m\n\u001b[32m    710\u001b[39m \n\u001b[32m    711\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     \u001b[43m_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPa_Sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "BLOCK_SIZE = 1024  # samples per callback\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_block = indata[:, 0]  # ‡πÉ‡∏ä‡πâ channel ‡πÅ‡∏£‡∏Å\n",
    "    transcription = asr_model.transcribe(audio_block)\n",
    "    print(\">>\", transcription)\n",
    "\n",
    "print(\"üé§ Start capturing (Ctrl+C to stop)\")\n",
    "with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, dtype=\"float32\",\n",
    "                    blocksize=BLOCK_SIZE, callback=audio_callback):\n",
    "    while True:\n",
    "        sd.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3dc919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method transcribe in module nemo.collections.asr.models.rnnt_models:\n",
      "\n",
      "transcribe(audio: Union[str, List[str], numpy.ndarray, torch.utils.data.dataloader.DataLoader], batch_size: int = 4, return_hypotheses: bool = False, partial_hypothesis: Optional[List[ForwardRef('Hypothesis')]] = None, num_workers: int = 0, channel_selector: Union[int, Iterable[int], str, NoneType] = None, augmentor: omegaconf.dictconfig.DictConfig = None, verbose: bool = True, override_config: Optional[nemo.collections.asr.parts.mixins.transcription.TranscribeConfig] = None) -> Union[List[str], List[ForwardRef('Hypothesis')], Tuple[List[str]], Tuple[List[ForwardRef('Hypothesis')]]] method of nemo.collections.asr.models.rnnt_bpe_models.EncDecRNNTBPEModel instance\n",
      "    Uses greedy decoding to transcribe audio files. Use this method for debugging and prototyping.\n",
      "\n",
      "    Args:\n",
      "        audio: (a single or list) of paths to audio files or a np.ndarray/tensor audio array or path to a manifest file.\n",
      "            Can also be a dataloader object that provides values that can be consumed by the model.\n",
      "            Recommended length per file is between 5 and 25 seconds.                 But it is possible to pass a few hours long file if enough GPU memory is available.\n",
      "        batch_size: (int) batch size to use during inference.                 Bigger will result in better throughput performance but would use more memory.\n",
      "        return_hypotheses: (bool) Either return hypotheses or text\n",
      "            With hypotheses can do some postprocessing like getting timestamp or rescoring\n",
      "        partial_hypothesis: Optional[List['Hypothesis']] - A list of partial hypotheses to be used during rnnt\n",
      "            decoding. This is useful for streaming rnnt decoding. If this is not None, then the length of this\n",
      "            list should be equal to the length of the audio list.\n",
      "        num_workers: (int) number of workers for DataLoader\n",
      "        channel_selector (int | Iterable[int] | str): select a single channel or a subset of channels from multi-channel audio. If set to `'average'`, it performs averaging across channels. Disabled if set to `None`. Defaults to `None`. Uses zero-based indexing.\n",
      "        augmentor: (DictConfig): Augment audio samples during transcription if augmentor is applied.\n",
      "        verbose: (bool) whether to display tqdm progress bar\n",
      "        override_config: (Optional[TranscribeConfig]) override transcription config pre-defined by the user.\n",
      "            **Note**: All other arguments in the function will be ignored if override_config is passed.\n",
      "            You should call this argument as `model.transcribe(audio, override_config=TranscribeConfig(...))`.\n",
      "\n",
      "    Returns:\n",
      "        Returns a tuple of 2 items -\n",
      "        * A list of greedy transcript texts / Hypothesis\n",
      "        * An optional list of beam search transcript texts / Hypothesis / NBestHypothesis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(asr_model.transcribe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb4b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# from pathlib import Path\n",
    "# import nemo.collections.asr as nemo_asr\n",
    "# import torch\n",
    "# import librosa\n",
    "# import soundfile as sf\n",
    "\n",
    "# def prepare_audio(input_path, target_sr=16000):\n",
    "#     input_path = Path(input_path)\n",
    "#     if not input_path.exists():\n",
    "#         raise FileNotFoundError(f\"File not found: {input_path}\")\n",
    "\n",
    "#     supported_formats = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac', '.webm']\n",
    "#     if input_path.suffix.lower() not in supported_formats:\n",
    "#         raise ValueError(f\"Unsupported format: {input_path.suffix}. Supported formats are: {supported_formats}\")\n",
    "\n",
    "#     processed_path = f\"processed_{input_path.stem}.wav\"\n",
    "\n",
    "#     y, sr = librosa.load(str(input_path), sr=None)\n",
    "#     if y is None:\n",
    "#         raise IOError(\"Failed to load audio file.\")\n",
    "\n",
    "#     duration = len(y) / sr\n",
    "\n",
    "#     if sr != target_sr:\n",
    "#         y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "#     y = y / (max(abs(y)) + 1e-8)\n",
    "#     sf.write(processed_path, y, target_sr)\n",
    "    \n",
    "#     return processed_path, duration\n",
    "\n",
    "# processed_path, duration = prepare_audio(input_path='audio/recorded_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a18372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πÇ‡∏Ñ‡∏£‡πÇ‡∏ü‡∏ô... ‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Exception ignored from cffi callback <function _StreamBase.__init__.<locals>.callback_ptr at 0x0000024A4239E980>:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\sounddevice.py\", line 857, in callback_ptr\n",
      "    return _wrap_callback(callback, data, frames, time, status)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\sounddevice.py\", line 2762, in _wrap_callback\n",
      "    callback(*args)\n",
      "  File \"C:\\Users\\TDGE0049\\AppData\\Local\\Temp\\ipykernel_23152\\3708563793.py\", line 31, in callback\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\nemo\\collections\\asr\\models\\rnnt_models.py\", line 280, in transcribe\n",
      "    return super().transcribe(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\mixins\\transcription.py\", line 281, in transcribe\n",
      "    for processed_outputs in generator:\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\mixins\\transcription.py\", line 391, in transcribe_generator\n",
      "    model_outputs = self._transcribe_forward(test_batch, transcribe_cfg)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\nemo\\collections\\asr\\models\\rnnt_models.py\", line 899, in _transcribe_forward\n",
      "    encoded, encoded_len = self.forward(input_signal=batch[0], input_signal_length=batch[1])\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\nemo\\core\\classes\\common.py\", line 1078, in wrapped_call\n",
      "    instance._validate_input_types(input_types=input_types, ignore_collections=self.ignore_collections, **kwargs)\n",
      "  File \"d:\\Git\\thai_voice_transcription\\.venv\\Lib\\site-packages\\nemo\\core\\classes\\common.py\", line 246, in _validate_input_types\n",
      "    raise TypeError(\n",
      "TypeError: Input shape mismatch occured for input_signal in module EncDecRNNTBPEModel : \n",
      "Input shape expected = (batch, time) | \n",
      "Input shape found : torch.Size([1, 1, 16000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ‡πÄ‡∏£‡∏¥‡πà‡∏° stream\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sd.InputStream(channels=\u001b[32m1\u001b[39m, samplerate=sample_rate, blocksize=block_size, callback=callback):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å device\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # ‡πÇ‡∏´‡∏•‡∏î model\n",
    "# asr_model = nemo_asr.models.ASRModel.from_pretrained(\n",
    "#     model_name=\"scb10x/typhoon-asr-realtime\",\n",
    "#     map_location=device\n",
    "# )\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á\n",
    "sample_rate = 16000  # Typhoon model ‡πÉ‡∏ä‡πâ 16kHz\n",
    "block_duration = 1.0  # 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠ batch\n",
    "block_size = int(sample_rate * block_duration)\n",
    "\n",
    "print(\"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πÇ‡∏Ñ‡∏£‡πÇ‡∏ü‡∏ô... ‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î\")\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_data = indata[:, 0]  # ‡πÉ‡∏ä‡πâ channel ‡πÅ‡∏£‡∏Å\n",
    "    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Transcribe\n",
    "    with torch.no_grad():\n",
    "        transcription = asr_model.transcribe(audio_tensor)\n",
    "        print(\">>\", transcription)\n",
    "\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏° stream\n",
    "with sd.InputStream(channels=1, samplerate=sample_rate, blocksize=block_size, callback=callback):\n",
    "    while True:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thai-voice-transcription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
